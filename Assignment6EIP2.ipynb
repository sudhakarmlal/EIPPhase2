{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudhakarmlal/EIPPhase2/blob/master/Assignment6EIP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkfYNo9nDlOS",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 6 (EIP Phase2 : Session2)\n",
        "##Project LSTM\n",
        "\n",
        "\n",
        "###Description:\n",
        "\n",
        "####The project is  about generating 500 texts using LSTM Model.\n",
        "#### The LSTM  Model to be trained on a dataset from Project GutenBerg.The Dataset(Book) on which the LSTM Model to be trained on  is 'WonderLand.txt'\n",
        "\n",
        "\n",
        "#### All the punctuations to be removed from  the source text\n",
        "\n",
        "#### To train the model padded sequences to be used instead of random sequences of characters.\n",
        "\n",
        "\n",
        "### The Approach\n",
        "\n",
        "#### Step1: Read the file as raw text from wonderland.txt.Mounted in the google drive.\n",
        "\n",
        "####Step2:Remove the punctuations from the files\n",
        "\n",
        "####Step3:Convert all the unique characters used in the text to int.Also find the vocab count which is the unique characters used \n",
        "\n",
        "\n",
        "#### Step4: Split the source text by \"\\n\"  while splitting the source text check the max length of the splits produced.\n",
        "\n",
        "####Step5:Pad each split produced with the max length derived from the previoius step\n",
        "\n",
        "\n",
        "####Step6:Merge these padded sequences of splits take this as the clean text from which train data should be generated\n",
        "\n",
        "\n",
        "####Step7: Generate train data(X_train) by sliding  10 characters each time.Take sequence length as max length of the splits produced dervied in step4.\n",
        "\n",
        "\n",
        "####:Step8:Feed this Data to LSTM Model.\n",
        "\n",
        "\n",
        "####:Step9:One the Model is trained.Generate 500 character sequences by providing an input  of random index from the train data(X_train)\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ag2wAQg86PC",
        "colab_type": "text"
      },
      "source": [
        "### Import necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPhtjoXz8b3N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c46e83f5-c857-4a6c-e8d5-cf48c26e7455"
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZlo4dKAD8vv",
        "colab_type": "text"
      },
      "source": [
        "### Mount the groogle drive in google colab to read the wonderland file(to be used by LSTM Model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA6PzK3F9zUb",
        "colab_type": "code",
        "outputId": "946f5da1-56cd-479e-a1ea-597e7ab40662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRxAkMj49UYk",
        "colab_type": "text"
      },
      "source": [
        "### Load the file wonderland.txt and read the text in the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3LcqJi39FWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = \"/content/drive/My Drive/SequenceModels/LSTM/wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrBg76IsEE2L",
        "colab_type": "text"
      },
      "source": [
        "#### Check the length of raw text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U35V09h-u0H",
        "colab_type": "code",
        "outputId": "de880a09-0368-4e84-cf77-eac3e565b309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(raw_text))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "144437\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfSxtijuELTM",
        "colab_type": "text"
      },
      "source": [
        "####Check the unique characters used in the source text and also convert these characters to int\n",
        "####Note:integral values to be fed to LSTM model and not characters.Hence the characters to be converted to int"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF8K71lr_HTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEEMbgPxBIb8",
        "colab_type": "code",
        "outputId": "e1777bd1-d8f4-4cbe-c3a5-aea041c5cf84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(chars)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '0', '3', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCWJYLrOzsuV",
        "colab_type": "code",
        "outputId": "c93073d3-bd99-42aa-eec9-9cccc03a4f77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(char_to_int)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, '*': 7, ',': 8, '-': 9, '.': 10, '0': 11, '3': 12, ':': 13, ';': 14, '?': 15, '[': 16, ']': 17, '_': 18, 'a': 19, 'b': 20, 'c': 21, 'd': 22, 'e': 23, 'f': 24, 'g': 25, 'h': 26, 'i': 27, 'j': 28, 'k': 29, 'l': 30, 'm': 31, 'n': 32, 'o': 33, 'p': 34, 'q': 35, 'r': 36, 's': 37, 't': 38, 'u': 39, 'v': 40, 'w': 41, 'x': 42, 'y': 43, 'z': 44, '\\ufeff': 45}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpGtxnbhBPnV",
        "colab_type": "code",
        "outputId": "837b217c-a689-4586-cb32-3e3f2ad643f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  144437\n",
            "Total Vocab:  46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2dB3MaAEYB2",
        "colab_type": "text"
      },
      "source": [
        "#### Remove punctuations from the source text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AWO6Docqx2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5pPJYpnq0Ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_punct_text = \"\"\n",
        "for char in raw_text:\n",
        "   if char not in punctuations:\n",
        "       no_punct_text = no_punct_text + char"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K7OZshcrLr5",
        "colab_type": "code",
        "outputId": "7e7a6369-6f62-43c0-a05d-8dd2719d5410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(no_punct_text))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "136117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SFTBB2Y5eOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_punct_text=no_punct_text.replace(\"\\n\\n\",\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Azkg2_g7XtE",
        "colab_type": "code",
        "outputId": "0cb9b14b-8f42-4482-b04f-6586d6c3b268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(no_punct_text)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "135284"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxtB3KSyEbAF",
        "colab_type": "text"
      },
      "source": [
        "#### Check the  unique characters and corresponding int values after removal of punctuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBz6yG7p82US",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = sorted(list(set(no_punct_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI0yVGMK89hS",
        "colab_type": "code",
        "outputId": "9c7f03fd-bccd-40fe-bbd1-0e0776a82809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(chars)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '0', '3', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1YnONH48_4r",
        "colab_type": "code",
        "outputId": "b6533f53-f6b2-49f0-e909-da0aad675881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(char_to_int)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '0': 2, '3': 3, 'a': 4, 'b': 5, 'c': 6, 'd': 7, 'e': 8, 'f': 9, 'g': 10, 'h': 11, 'i': 12, 'j': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22, 't': 23, 'u': 24, 'v': 25, 'w': 26, 'x': 27, 'y': 28, 'z': 29, '\\ufeff': 30}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ld1WEMwEnHH",
        "colab_type": "text"
      },
      "source": [
        "#### Print total characters in the text and total unique characters after removal of punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5MUVIaTTQ1t",
        "colab_type": "code",
        "outputId": "4facfea6-1165-4bd8-dac0-399a0cebcbf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "n_chars = len(no_punct_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  135284\n",
            "Total Vocab:  31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r5PTcC69QS3",
        "colab_type": "text"
      },
      "source": [
        "#### Split the source text by \"\\n\" to get the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGJtB5ccmSJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = no_punct_text.lower().split(\"\\n\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py1u7_H8rs9V",
        "colab_type": "code",
        "outputId": "86dedac0-a234-4574-eb62-063696db0500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(corpus)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2512"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8M4Pj80E4ko",
        "colab_type": "text"
      },
      "source": [
        "#### Further remove empty lines from the source text and make the final corpus.While making the final corpus just check the max length of all the splits produced to be used for padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owBpMYPnmjVh",
        "colab_type": "code",
        "outputId": "f65fd437-0df3-4194-ff35-bc6682529b7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "max = max=len(corpus[0])\n",
        "#len(corpus)\n",
        "ctr =0\n",
        "final_corpus = []\n",
        "#final_corpus = [0 for i in range(len(corpus))]\n",
        "for i in range(len(corpus)):\n",
        "  #print(len(corpus[i]))\n",
        "  if len(corpus[i]) != 0:\n",
        "    final_corpus.append(corpus[i])\n",
        "  if len(corpus[i]) >= max:\n",
        "    max=len(corpus[i])  \n",
        "  \n",
        "\n",
        "print(max)\n",
        "print(len(final_corpus))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "73\n",
            "2481\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDI5zVsDE9-c",
        "colab_type": "text"
      },
      "source": [
        "#### Pad each split by the max length derived in the previous step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cVefP5v_Yjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded_corpus =[]\n",
        "for i in range(len(final_corpus)):\n",
        "  pad=73-len(final_corpus[i])\n",
        "  #print(\"Pad\",pad)\n",
        "  #print(final_corpus[i])\n",
        "  padded =str(final_corpus[i]).ljust(73,\" \")\n",
        "  #print(\"afterpadding\",len(padded))\n",
        "  padded_corpus.append(padded)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIyom9mbFE0r",
        "colab_type": "text"
      },
      "source": [
        "#### Verify if padding is proper by printing the length of 10 elements of padded corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oOPl3pKBNV4",
        "colab_type": "code",
        "outputId": "c94c8926-1192-44a0-eef8-dc76b43b4c4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(len(padded_corpus[i]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "73\n",
            "73\n",
            "73\n",
            "73\n",
            "73\n",
            "73\n",
            "73\n",
            "73\n",
            "73\n",
            "73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5CJhywQFR2A",
        "colab_type": "text"
      },
      "source": [
        "#### Merge the Padded sequence to create a clean text which is the  free from punctionations,empty lines and is padded(padded after the split on \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbxiOoLycigY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_text=\"\"\n",
        "for i in range(len(padded_corpus)):\n",
        "  clean_text = clean_text+padded_corpus[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlVKp8odc4Y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = sorted(list(set(clean_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTPVw6qFc9vj",
        "colab_type": "code",
        "outputId": "7a32fa3c-b2a0-442f-ac06-2e7403dc426c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "n_chars = len(clean_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  181113\n",
            "Total Vocab:  30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL9OmUw6MlGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(len(padded_corpus)-1):\n",
        "  seq_in=padded_corpus[i]\n",
        "  seq_out=padded_corpus[i+1][0]\n",
        "  dataX.append([char_to_int[char] for char in seq_in])\n",
        "  dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sdf9SY5HphS",
        "colab_type": "text"
      },
      "source": [
        "#### Form train data from the clean text produced in the previous step.\n",
        "#### While training take a window slide of 10 characters and sequence length of  73 which is max length of the splits(derived in the previous step) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq0LHFOGCCZ8",
        "colab_type": "code",
        "outputId": "b77231b4-19e2-4c27-852c-c3d9fa40baf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 73\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 10):\n",
        "\tseq_in = clean_text[i:i + seq_length]\n",
        "\tseq_out = clean_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "\n",
        "#cntr = 0\n",
        "#for i in range(0, n_chars - seq_length, 1):\n",
        "  #print(\"Hi\")\n",
        "  #if cntr < 20:\n",
        "    #print(raw_text[i:i + seq_length])\n",
        "    #print(len(raw_text[i:i + seq_length]))\n",
        "    #print(raw_text[i + seq_length])\n",
        "  #cntr = cntr +1\n",
        "#print(cntr)\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  18104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgeUvb4gIZ7L",
        "colab_type": "text"
      },
      "source": [
        "#### Reshape the train data for feeding to LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUr2kAFEKX9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, 73, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih2U9ka5Ikp2",
        "colab_type": "text"
      },
      "source": [
        "#### Build the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7qYSdIyLRIY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "ea26dfc3-10fe-4d97-99d0-42a408f91554"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True,dropout=0.1))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0727 10:42:05.182772 140188924004224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0727 10:42:05.219854 140188924004224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0727 10:42:05.373200 140188924004224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0727 10:42:05.627557 140188924004224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0727 10:42:05.638804 140188924004224 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFAImQGNIogH",
        "colab_type": "text"
      },
      "source": [
        "#### Define the check point to save the best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy8JxNUnLqc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# define the checkpoint\n",
        "#filepath=\"/content/drive/My Drive/EIPSession2/SequenceModels/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "filepath=\"/content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW3jDERsIvrD",
        "colab_type": "text"
      },
      "source": [
        "#### Run the Model (Batch Size 64 and number of epochs 100)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWpb-shBMgAH",
        "colab_type": "code",
        "outputId": "ad629d84-a3d5-4ed7-e1ed-0f486c90df76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#model.fit(X, y, epochs=5, batch_size=128, callbacks=callbacks_list)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, epochs=100, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0727 10:42:24.854573 140188924004224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0727 10:42:24.884869 140188924004224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0727 10:42:25.035172 140188924004224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "18104/18104 [==============================] - 60s 3ms/step - loss: 2.3520\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.35201, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 2/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 2.2473\n",
            "\n",
            "Epoch 00002: loss improved from 2.35201 to 2.24733, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 3/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 2.1593\n",
            "\n",
            "Epoch 00003: loss improved from 2.24733 to 2.15935, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 4/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 2.0464\n",
            "\n",
            "Epoch 00004: loss improved from 2.15935 to 2.04641, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 5/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 1.9776\n",
            "\n",
            "Epoch 00005: loss improved from 2.04641 to 1.97758, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 6/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 1.9316\n",
            "\n",
            "Epoch 00006: loss improved from 1.97758 to 1.93158, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 7/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 1.8909\n",
            "\n",
            "Epoch 00007: loss improved from 1.93158 to 1.89089, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 8/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 1.8442\n",
            "\n",
            "Epoch 00008: loss improved from 1.89089 to 1.84418, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 9/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 1.8031\n",
            "\n",
            "Epoch 00009: loss improved from 1.84418 to 1.80306, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 10/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 1.7633\n",
            "\n",
            "Epoch 00010: loss improved from 1.80306 to 1.76332, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 11/100\n",
            "18104/18104 [==============================] - 56s 3ms/step - loss: 1.7338\n",
            "\n",
            "Epoch 00011: loss improved from 1.76332 to 1.73385, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 12/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 1.6979\n",
            "\n",
            "Epoch 00012: loss improved from 1.73385 to 1.69791, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 13/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 1.6640\n",
            "\n",
            "Epoch 00013: loss improved from 1.69791 to 1.66401, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 14/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 1.6265\n",
            "\n",
            "Epoch 00014: loss improved from 1.66401 to 1.62649, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 15/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 1.5912\n",
            "\n",
            "Epoch 00015: loss improved from 1.62649 to 1.59118, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 16/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 1.5432\n",
            "\n",
            "Epoch 00016: loss improved from 1.59118 to 1.54319, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 17/100\n",
            "18104/18104 [==============================] - 56s 3ms/step - loss: 1.4920\n",
            "\n",
            "Epoch 00017: loss improved from 1.54319 to 1.49205, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 18/100\n",
            "18104/18104 [==============================] - 56s 3ms/step - loss: 1.4499\n",
            "\n",
            "Epoch 00018: loss improved from 1.49205 to 1.44988, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 19/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 1.3944\n",
            "\n",
            "Epoch 00019: loss improved from 1.44988 to 1.39443, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 20/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 1.3547\n",
            "\n",
            "Epoch 00020: loss improved from 1.39443 to 1.35470, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 21/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 1.3021\n",
            "\n",
            "Epoch 00021: loss improved from 1.35470 to 1.30208, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 22/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 1.2537\n",
            "\n",
            "Epoch 00022: loss improved from 1.30208 to 1.25368, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 23/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 1.1928\n",
            "\n",
            "Epoch 00023: loss improved from 1.25368 to 1.19283, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 24/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 1.1418\n",
            "\n",
            "Epoch 00024: loss improved from 1.19283 to 1.14178, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 25/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 1.0851\n",
            "\n",
            "Epoch 00025: loss improved from 1.14178 to 1.08505, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 26/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 1.0314\n",
            "\n",
            "Epoch 00026: loss improved from 1.08505 to 1.03140, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 27/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.9937\n",
            "\n",
            "Epoch 00027: loss improved from 1.03140 to 0.99366, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 28/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.9247\n",
            "\n",
            "Epoch 00028: loss improved from 0.99366 to 0.92470, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 29/100\n",
            "18104/18104 [==============================] - 60s 3ms/step - loss: 0.8637\n",
            "\n",
            "Epoch 00029: loss improved from 0.92470 to 0.86365, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 30/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.8097\n",
            "\n",
            "Epoch 00030: loss improved from 0.86365 to 0.80969, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 31/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.7543\n",
            "\n",
            "Epoch 00031: loss improved from 0.80969 to 0.75434, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 32/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.6985\n",
            "\n",
            "Epoch 00032: loss improved from 0.75434 to 0.69847, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 33/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.6518\n",
            "\n",
            "Epoch 00033: loss improved from 0.69847 to 0.65184, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 34/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.6027\n",
            "\n",
            "Epoch 00034: loss improved from 0.65184 to 0.60271, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 35/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.5526\n",
            "\n",
            "Epoch 00035: loss improved from 0.60271 to 0.55259, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 36/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.5065\n",
            "\n",
            "Epoch 00036: loss improved from 0.55259 to 0.50647, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 37/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.4806\n",
            "\n",
            "Epoch 00037: loss improved from 0.50647 to 0.48059, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 38/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.4304\n",
            "\n",
            "Epoch 00038: loss improved from 0.48059 to 0.43039, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 39/100\n",
            "18104/18104 [==============================] - 59s 3ms/step - loss: 0.3985\n",
            "\n",
            "Epoch 00039: loss improved from 0.43039 to 0.39854, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 40/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.3656\n",
            "\n",
            "Epoch 00040: loss improved from 0.39854 to 0.36563, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 41/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.3387\n",
            "\n",
            "Epoch 00041: loss improved from 0.36563 to 0.33875, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 42/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.3202\n",
            "\n",
            "Epoch 00042: loss improved from 0.33875 to 0.32017, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 43/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.3037\n",
            "\n",
            "Epoch 00043: loss improved from 0.32017 to 0.30375, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 44/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.2729\n",
            "\n",
            "Epoch 00044: loss improved from 0.30375 to 0.27294, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 45/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.2514\n",
            "\n",
            "Epoch 00045: loss improved from 0.27294 to 0.25137, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 46/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.2461\n",
            "\n",
            "Epoch 00046: loss improved from 0.25137 to 0.24612, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 47/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.2363\n",
            "\n",
            "Epoch 00047: loss improved from 0.24612 to 0.23633, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 48/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.2112\n",
            "\n",
            "Epoch 00048: loss improved from 0.23633 to 0.21119, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 49/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.2030\n",
            "\n",
            "Epoch 00049: loss improved from 0.21119 to 0.20303, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 50/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.2001\n",
            "\n",
            "Epoch 00050: loss improved from 0.20303 to 0.20006, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 51/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1771\n",
            "\n",
            "Epoch 00051: loss improved from 0.20006 to 0.17712, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 52/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.1765\n",
            "\n",
            "Epoch 00052: loss improved from 0.17712 to 0.17654, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 53/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.1733\n",
            "\n",
            "Epoch 00053: loss improved from 0.17654 to 0.17325, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 54/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.1594\n",
            "\n",
            "Epoch 00054: loss improved from 0.17325 to 0.15942, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 55/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1657\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.15942\n",
            "Epoch 56/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1682\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.15942\n",
            "Epoch 57/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1487\n",
            "\n",
            "Epoch 00057: loss improved from 0.15942 to 0.14870, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 58/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1646\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.14870\n",
            "Epoch 59/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1480\n",
            "\n",
            "Epoch 00059: loss improved from 0.14870 to 0.14802, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 60/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.1395\n",
            "\n",
            "Epoch 00060: loss improved from 0.14802 to 0.13950, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 61/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1323\n",
            "\n",
            "Epoch 00061: loss improved from 0.13950 to 0.13234, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 62/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.1196\n",
            "\n",
            "Epoch 00062: loss improved from 0.13234 to 0.11961, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 63/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1793\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.11961\n",
            "Epoch 64/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1486\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.11961\n",
            "Epoch 65/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.1217\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.11961\n",
            "Epoch 66/100\n",
            "18104/18104 [==============================] - 59s 3ms/step - loss: 0.1163\n",
            "\n",
            "Epoch 00066: loss improved from 0.11961 to 0.11631, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 67/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.1274\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.11631\n",
            "Epoch 68/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.1191\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.11631\n",
            "Epoch 69/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1216\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.11631\n",
            "Epoch 70/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1147\n",
            "\n",
            "Epoch 00070: loss improved from 0.11631 to 0.11468, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 71/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.0993\n",
            "\n",
            "Epoch 00071: loss improved from 0.11468 to 0.09926, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 72/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1269\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.09926\n",
            "Epoch 73/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.1181\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.09926\n",
            "Epoch 74/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1162\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.09926\n",
            "Epoch 75/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1285\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.09926\n",
            "Epoch 76/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1019\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.09926\n",
            "Epoch 77/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1097\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.09926\n",
            "Epoch 78/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.0906\n",
            "\n",
            "Epoch 00078: loss improved from 0.09926 to 0.09061, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 79/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.1015\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.09061\n",
            "Epoch 80/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.0943\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.09061\n",
            "Epoch 81/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1665\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.09061\n",
            "Epoch 82/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1092\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.09061\n",
            "Epoch 83/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.0973\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.09061\n",
            "Epoch 84/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.0956\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.09061\n",
            "Epoch 85/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.0977\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.09061\n",
            "Epoch 86/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.0903\n",
            "\n",
            "Epoch 00086: loss improved from 0.09061 to 0.09034, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 87/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.1006\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.09034\n",
            "Epoch 88/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.0875\n",
            "\n",
            "Epoch 00088: loss improved from 0.09034 to 0.08752, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 89/100\n",
            "18104/18104 [==============================] - 58s 3ms/step - loss: 0.0850\n",
            "\n",
            "Epoch 00089: loss improved from 0.08752 to 0.08503, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 90/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.0979\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.08503\n",
            "Epoch 91/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.1093\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.08503\n",
            "Epoch 92/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.0993\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.08503\n",
            "Epoch 93/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.0796\n",
            "\n",
            "Epoch 00093: loss improved from 0.08503 to 0.07963, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 94/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.0799\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.07963\n",
            "Epoch 95/100\n",
            "18104/18104 [==============================] - 59s 3ms/step - loss: 0.0980\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.07963\n",
            "Epoch 96/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.0887\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.07963\n",
            "Epoch 97/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.0773\n",
            "\n",
            "Epoch 00097: loss improved from 0.07963 to 0.07731, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n",
            "Epoch 98/100\n",
            "18104/18104 [==============================] - 56s 3ms/step - loss: 0.1133\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.07731\n",
            "Epoch 99/100\n",
            "18104/18104 [==============================] - 57s 3ms/step - loss: 0.0989\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.07731\n",
            "Epoch 100/100\n",
            "18104/18104 [==============================] - 60s 3ms/step - loss: 0.0716\n",
            "\n",
            "Epoch 00100: loss improved from 0.07731 to 0.07163, saving model to /content/drive/My Drive/SequenceModels/LSTM/weights-improv.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7ff8fb5f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1ljKN5QI9vM",
        "colab_type": "text"
      },
      "source": [
        "#### Once Model is trained load the best weights of the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPDhArZXTwo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the network weights\n",
        "filename = \"/content/drive/My Drive/EIPSession2/SequenceModels/weights-improvement.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N3mrtM4JCH-",
        "colab_type": "text"
      },
      "source": [
        "####  Convert int to characters to be used to generate text of 500 character sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkDHNtpPT3uK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zjd2-fxJQZE",
        "colab_type": "text"
      },
      "source": [
        "#### Predict 500 character sequences:\n",
        "\n",
        "####  Generate 500 character sequences based on the training data input fed to the model.\n",
        "#### A random seed of training data is fed to the Model as input for predicting 500 character sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pJSxFqbUArB",
        "colab_type": "code",
        "outputId": "72445e53-fc2c-40dd-8392-9a234e03cd63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "print(\"Start is\",start)\n",
        "pattern = dataX[start]\n",
        "\n",
        "print(\"Pattern is\",pattern)\n",
        "print(\"Pattern length is\",len(pattern))\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tprint(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start is 6773\n",
            "Pattern is [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 23, 22, 0, 11, 15, 0, 16, 17, 22, 0, 3, 0, 21, 7, 20, 18, 7, 16, 22, 0, 11, 0, 22, 7, 14, 14, 0, 27, 17, 23, 0, 21, 3, 11, 6, 0, 3, 14, 11, 5, 7, 0, 11, 15, 0, 3, 11, 15, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Pattern length is 73\n",
            "Seed:\n",
            "\"               but im not a serpent i tell you said alice im aim a         \"\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "a\n",
            "n\n",
            "i\n",
            "c\n",
            "e\n",
            " \n",
            "t\n",
            "a\n",
            "n\n",
            "d\n",
            " \n",
            "n\n",
            "n\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "d\n",
            "a\n",
            "k\n",
            "e\n",
            "e\n",
            " \n",
            "h\n",
            "v\n",
            " \n",
            "a\n",
            "l\n",
            "l\n",
            "c\n",
            "e\n",
            " \n",
            "f\n",
            "a\n",
            "n\n",
            "l\n",
            " \n",
            "b\n",
            "n\n",
            "d\n",
            " \n",
            "t\n",
            "a\n",
            "s\n",
            " \n",
            "a\n",
            " \n",
            "b\n",
            "u\n",
            "w\n",
            "i\n",
            " \n",
            "t\n",
            "n\n",
            "t\n",
            "n\n",
            " \n",
            "h\n",
            "t\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "q\n",
            "i\n",
            "g\n",
            "l\n",
            "t\n",
            " \n",
            "i\n",
            "i\n",
            "a\n",
            "d\n",
            " \n",
            "i\n",
            "o\n",
            "r\n",
            "s\n",
            "t\n",
            "e\n",
            "d\n",
            " \n",
            "a\n",
            " \n",
            "b\n",
            "o\n",
            "d\n",
            " \n",
            "w\n",
            "h\n",
            "e\n",
            "n\n",
            "e\n",
            " \n",
            "w\n",
            "h\n",
            "e\n",
            " \n",
            "l\n",
            "a\n",
            "d\n",
            " \n",
            "b\n",
            "u\n",
            "c\n",
            "h\n",
            "e\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "a\n",
            " \n",
            "b\n",
            "i\n",
            "i\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            "y\n",
            " \n",
            "t\n",
            "a\n",
            "i\n",
            "d\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "h\n",
            "a\n",
            "t\n",
            "t\n",
            "e\n",
            "r\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "a\n",
            "n\n",
            "d\n",
            " \n",
            "c\n",
            "e\n",
            "g\n",
            " \n",
            "n\n",
            "n\n",
            " \n",
            "a\n",
            " \n",
            "d\n",
            "a\n",
            "m\n",
            "d\n",
            " \n",
            "h\n",
            "v\n",
            " \n",
            "t\n",
            "c\n",
            "l\n",
            "d\n",
            " \n",
            "a\n",
            "l\n",
            "i\n",
            "c\n",
            "e\n",
            " \n",
            "y\n",
            "h\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "t\n",
            "o\n",
            "o\n",
            "y\n",
            " \n",
            "o\n",
            "i\n",
            "k\n",
            "e\n",
            " \n",
            "t\n",
            "o\n",
            " \n",
            "s\n",
            "r\n",
            "t\n",
            "t\n",
            " \n",
            "t\n",
            "f\n",
            "e\n",
            " \n",
            "s\n",
            "f\n",
            "l\n",
            "l\n",
            "o\n",
            "e\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "o\n",
            "t\n",
            "s\n",
            " \n",
            "a\n",
            "n\n",
            "y\n",
            " \n",
            "y\n",
            "h\n",
            "a\n",
            "t\n",
            " \n",
            "i\n",
            "i\n",
            "s\n",
            " \n",
            "f\n",
            "e\n",
            "a\n",
            "d\n",
            " \n",
            "s\n",
            "o\n",
            " \n",
            "h\n",
            "t\n",
            " \n",
            "f\n",
            "a\n",
            "n\n",
            " \n",
            "h\n",
            "n\n",
            " \n",
            "h\n",
            "a\n",
            "r\n",
            " \n",
            "h\n",
            "a\n",
            "w\n",
            "t\n",
            " \n",
            "w\n",
            "o\n",
            " \n",
            "b\n",
            "r\n",
            "d\n",
            " \n",
            "c\n",
            "a\n",
            "g\n",
            "u\n",
            " \n",
            "t\n",
            "a\n",
            "n\n",
            "d\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "i\n",
            "i\n",
            "s\n",
            "e\n",
            " \n",
            "i\n",
            "e\n",
            "r\n",
            " \n",
            "e\n",
            "o\n",
            "a\n",
            "u\n",
            " \n",
            "i\n",
            "i\n",
            "t\n",
            " \n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}